{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.python.keras.applications.densenet import DenseNet\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Input, Flatten, Dropout\n",
    "from tensorflow.keras.layers import concatenate, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets, layers\n",
    "import matplotlib.pyplot as plt\n",
    "# import configparser\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pathlib\n",
    "import argparse\n",
    "import sys\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "docker container rm sl-$SLNUM ; bash ./swarm-learning/bin/run-sl        \\\n",
    "    --name=sl-$SLNUM                         \\\n",
    "    --network sl-net             \\\n",
    "    --sl-platform=TF                   \\\n",
    "    --host-ip=sl-$SLNUM                \\\n",
    "    --sn-ip=sn-$SNNUM                   \\\n",
    "    --data-dir=\"/home/yudonghan/storage/NIHCHEST\"  \\\n",
    "    --model-dir=\"/home/yudonghan/storage/NIHCHEST/SplitResult/$SLNUM\"  \\\n",
    "    --model-program=train.py        \\\n",
    "    --apls-ip apls                 \\\n",
    "    --gpu=$GPU                        \\\n",
    "    -serverAddress spire-server            \\\n",
    "    -genJoinToken  \\\n",
    "    -e SLNUM=$SLNUM  \\\n",
    "    -e SNNUM=$SNNUM  \\\n",
    "    -e WEIGHTAGE=$WEIGHTAGE\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@keras_export('keras.applications.densenet.DenseNet49',\n",
    "              'keras.applications.DenseNet49')\n",
    "def DenseNet49(include_top=False,\n",
    "               weights=None,\n",
    "               input_tensor=None,\n",
    "               input_shape=None,\n",
    "               pooling=None,\n",
    "               classes=1000):\n",
    "    \"\"\"Instantiates the Densenet49 architecture.\"\"\"\n",
    "    return DenseNet([4, 4, 8, 6], include_top, weights, input_tensor, input_shape, pooling, classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = json.load(open('config.json'))\n",
    "print(config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LR = config['learning_rate']\n",
    "EPOCHS = config['epochs']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "MODEL = config['model']\n",
    "DROPOUT = config['dropout']\n",
    "BASE_NAME = config['base_name']\n",
    "SWARM_LEARNING = config['sl']\n",
    "MIN_PEERS = config['min_peers']\n",
    "SYNC_INTERVAL = config['sync_interval']\n",
    "WEIGHTAGE = os.getenv('WEIGHTAGE', 50)\n",
    "DS_PATH = 'cifar10.npz'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-n', '--name', type=str, default=BASE_NAME)\n",
    "# parser.add_argument('-p', '--path', type=str, default=DS_PATH)\n",
    "# args = parser.parse_args()\n",
    "# if args.name:\n",
    "#     BASE_NAME = args.name\n",
    "# if args.path:\n",
    "#     DS_PATH = args.path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example Usage:\n",
    "\n",
    "```bash\n",
    "python3 main.py -n cifar10-CL-norm -p data/cifar10.npz\n",
    "python3 main.py -n cifar10-LL-norm -p SplitResults-1-1-1/1/cifar10.npz\n",
    "python3 main.py -n cifar10-LL-norm -p SplitResults-1-1-1/2/cifar10.npz\n",
    "python3 main.py -n cifar10-LL-norm -p SplitResults-1-1-1/3/cifar10.npz\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loaded = np.load(DS_PATH)\n",
    "train_images = loaded['train_images']\n",
    "train_labels = loaded['train_labels']\n",
    "test_images = loaded['test_images']\n",
    "test_labels = loaded['test_labels']\n",
    "train_num = train_images.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NAME = BASE_NAME + f'_{MODEL}_lr_{LR}_epochs_{EPOCHS}_batch_size_{BATCH_SIZE}_dropout_{DROPOUT}_train_num_{train_num}_W{WEIGHTAGE}'\n",
    "print(NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_labels = tf.squeeze(tf.one_hot(train_labels, 10))\n",
    "test_labels = tf.squeeze(tf.one_hot(test_labels, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_images.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalize pixel values to be between -1 and 1\n",
    "if config['norm']:\n",
    "    # mean = config['mean'] if 'mean' in config else train_images.mean()\n",
    "    # std = config['std'] if 'std' in config else train_images.std()\n",
    "    # train_images = (train_images - mean) / std\n",
    "    # test_images = (test_images - mean) / std\n",
    "    train_images = tf.image.per_image_standardization(train_images)\n",
    "    test_images = tf.image.per_image_standardization(test_images)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_images.shape, train_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(MODEL)\n",
    "# if MODEL == 'custom_cnn':\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.InputLayer(input_shape=(32, 32, 3)))\n",
    "#     model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Dropout(DROPOUT))\n",
    "#     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     # model.add(layers.Dropout(DROPOUT))\n",
    "#     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(layers.GlobalAveragePooling2D())\n",
    "#     model.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "#     model.add(layers.Dropout(DROPOUT))\n",
    "#     model.add(layers.Dense(10, activation='softmax'))\n",
    "#     model.build()\n",
    "# elif MODEL == 'efficientnetb0':\n",
    "#     base_model = tf.keras.applications.EfficientNetB0(weights=None, pooling='avg', input_shape=(32, 32, 3))\n",
    "#     model = tf.keras.Sequential([\n",
    "#         base_model,\n",
    "#         layers.Dense(10, activation='softmax')\n",
    "#     ])\n",
    "#     model.build(input_shape=base_model.input_shape)\n",
    "# elif MODEL == 'densenet49':\n",
    "#     base_model = DenseNet49(weights=None, pooling='avg', input_shape=(32, 32, 3))\n",
    "#     model = tf.keras.Sequential([\n",
    "#         base_model,\n",
    "#         layers.Dense(10, activation='softmax')\n",
    "#     ])\n",
    "#     model.build(input_shape=base_model.input_shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base_model = tf.keras.applications.DenseNet121(weights=None, input_shape=(32, 32, 3), include_top=False, pooling='avg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Dense(\n",
    "        units=10,\n",
    "        activation='softmax',\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "    )\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pathlib.Path('./history').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('./models').mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=f'./logs/{NAME}', profile_batch=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'./models/{NAME}.h5', save_best_only=False, save_weights_only=False),\n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if SWARM_LEARNING:\n",
    "    step_per_epoch = (train_num + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    from swarm import SwarmCallback\n",
    "    real_sync_interval = SYNC_INTERVAL  #*step_per_epoch\n",
    "    NAME += f'_INTERVAL_{real_sync_interval}'\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    print(\"SWARM-LERANING!\")\n",
    "    print(f\"+++ REAL SYNC INTERVAL: {real_sync_interval}\")\n",
    "    print(f\"+++ MIN PEERS: {MIN_PEERS}\")\n",
    "    print(f\"+++ NODE WEIGHTAGE: {WEIGHTAGE}\")\n",
    "    print(f\"+++ BATCH SIZE: {BATCH_SIZE}\")\n",
    "    print(f\"+++ MODEL NAME: {BASE_NAME}\")\n",
    "    print(f\"+++ TF VERSION: {tf.version.VERSION}\")\n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    swarmCallback = SwarmCallback(\n",
    "        sync_interval=real_sync_interval,\n",
    "        min_peers=MIN_PEERS,\n",
    "        node_weightage=WEIGHTAGE,\n",
    "        val_data=(test_images, test_labels),\n",
    "        val_batch_size=BATCH_SIZE,\n",
    "        model_name=BASE_NAME,\n",
    "        use_adaptive_sync=False\n",
    "    )\n",
    "    \n",
    "    callbacks.append(swarmCallback)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set data augmentation\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.125,\n",
    "    height_shift_range=0.125,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    ")\n",
    "\n",
    "datagen.fit(train_images)\n",
    "train_flow = datagen.flow(train_images, train_labels, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(train_flow, epochs=EPOCHS, validation_data=(test_images, test_labels), callbacks=callbacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "json.dump(history.history, open(f'./history/{NAME}.json', 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('python39': conda)"
  },
  "interpreter": {
   "hash": "a4cb7d616bfb3a17907725a4a53e58273ef9a6a96e08090c3a7874ada81a7810"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}